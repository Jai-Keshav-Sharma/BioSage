{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd66927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/preprocess_docs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/preprocess_docs.py\n",
    "\n",
    "import fitz\n",
    "from pathlib import Path\n",
    "from typing import Dict, List \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.docstore.document import Document \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from src.config import (\n",
    "    DOCUMENTS_PATH,\n",
    "    QDRANT_PATH,\n",
    "    COLLECTION_NAME,\n",
    "    EMBEDDING_MODEL,\n",
    "    CHUNK_SIZE,\n",
    "    CHUNK_OVERLAP,\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "# PDF Extraction with Rich Metadata\n",
    "def extract_text_from_pdf(pdf_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract text and metadata from research paper PDFs\n",
    "    Extracts: PMC ID, title, sections, page numbers\n",
    "    \"\"\"\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Extract PMC ID and title from filename\n",
    "    filename = pdf_path.stem\n",
    "    pmc_id = filename.split(\"_\")[0] if \"_\" in filename else filename\n",
    "    title = filename.split(\"_\", 1)[1] if \"_\" in filename else filename\n",
    "\n",
    "    # Try to extract the PDF metadata\n",
    "    pdf_metadata = doc.metadata\n",
    "\n",
    "    # Extract text page by page\n",
    "    full_text = []\n",
    "    pages_data = []\n",
    "\n",
    "    for page_num, page in enumerate(doc, 1):\n",
    "        text = page.get_text(\"text\")\n",
    "\n",
    "        # Clean up PDF artifacts \n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Remove excessive newlines\n",
    "        text = re.sub(r'(?<=[a-z])-\\n(?=[a-z])', '', text)  # Fix hyphenation\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # Normalize spaces and tabs to single space, preserving newlines\n",
    "\n",
    "        full_text.append(text)\n",
    "        pages_data.append({\n",
    "            \"page_num\": page_num, \n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    combined_text = \"\\n\\n\".join(full_text)\n",
    "\n",
    "    # Detect organism/subject from title (useful for filtering later)\n",
    "    organisms = []\n",
    "    organisms_list = [\n",
    "        'Arabidopsis', 'Burkholderia', 'Brassica', 'Penicillium',\n",
    "        'Cohnella', 'Pseudomonas', 'Caenorhabditis', 'elegans',\n",
    "        'human', 'mouse', 'rat'\n",
    "    ]\n",
    "    for organism in organisms_list:\n",
    "        if organism.lower() in title.lower() or organism.lower() in combined_text[:2000].lower():\n",
    "            organisms.append(organism)\n",
    "    \n",
    "    # Detect research type from title\n",
    "    research_types = []\n",
    "    if any(word in title.lower() for word in ['genomic', 'genome', 'gene']):\n",
    "        research_types.append('genomic')\n",
    "    if any(word in title.lower() for word in ['phenotypic', 'morphology']):\n",
    "        research_types.append('phenotypic')\n",
    "    if any(word in title.lower() for word in ['meta-analysis', 'review']):\n",
    "        research_types.append('meta-analysis')\n",
    "    if any(word in title.lower() for word in ['biofilm', 'microbial']):\n",
    "        research_types.append('microbiology')\n",
    "    \n",
    "    return {\n",
    "        'text': combined_text, \n",
    "        'pages_data': pages_data, \n",
    "        'metadata': {\n",
    "            'source': str(pdf_path), \n",
    "            'pmc_id': pmc_id, \n",
    "            'title': title, \n",
    "            'filename': pdf_path.stem,\n",
    "            'total_pages': len(doc), \n",
    "            'organisms': organisms if organisms else ['unknown'], \n",
    "            'research_types': research_types if research_types else ['general'], \n",
    "            'pdf_author': pdf_metadata.get('author', ''), \n",
    "            'pdf_subject': pdf_metadata.get('subject', ''),\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Load all dorcuments \n",
    "def load_all_documents() -> List[Document]:\n",
    "    \"\"\"Load and process all PDFs from the documents folder.\"\"\"\n",
    "\n",
    "    documents = []\n",
    "    pdf_files = sorted(list(DOCUMENTS_PATH.glob(\"*.pdf\")))\n",
    "\n",
    "    print(f\"\\nüìÑ Found {len(pdf_files)} PDF files\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        try:\n",
    "            print(f\"[{i:2d}/{len(pdf_files)}] Processing: {pdf_path.name[:65]}...\")\n",
    "            extracted = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            # Create Langchain Document \n",
    "            doc = Document(\n",
    "                page_content=extracted['text'],\n",
    "                metadata = extracted['metadata']\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "            # Show extracted Metadata\n",
    "            organisms = extracted['metadata']['organisms']\n",
    "            types = extracted['metadata']['research_types']\n",
    "            print(f\"        ‚îî‚îÄ Organisms: {', '.join(organisms[:3])}\")\n",
    "            print(f\"        ‚îî‚îÄ Types: {', '.join(types)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(documents)} documents\\n\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Chunk documents with page tracking\n",
    "def chunk_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split document into chunks while preserving metadata\n",
    "    Adds chunk_id and estimates page numbers\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP, \n",
    "        length_function=len, \n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Enrich chunk metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata['chunk_id'] = i\n",
    "\n",
    "        # Estimate which section (beginning, middle, or end of paper)\n",
    "        doc_length = len(chunk.page_content)\n",
    "        if i < len(chunks) * 0.2:\n",
    "            chunk.metadata['section_position'] = 'introduction'\n",
    "        elif i > len(chunks) * 0.8:\n",
    "            chunk.metadata['section_position'] = 'conclusion'\n",
    "        else:\n",
    "            chunk.metadata['section_position'] = 'body'\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "    print(f\"   Average chunk size: {sum(len(c.page_content) for c in chunks) // len(chunks)} chars\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb99eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/create_vectorstore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/create_vectorstore.py\n",
    "\n",
    "\n",
    "import fitz\n",
    "from pathlib import Path\n",
    "from typing import Dict, List \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.docstore.document import Document \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from src.config import (\n",
    "    DOCUMENTS_PATH,\n",
    "    QDRANT_PATH,\n",
    "    COLLECTION_NAME,\n",
    "    EMBEDDING_MODEL,\n",
    "    CHUNK_SIZE,\n",
    "    CHUNK_OVERLAP,\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Create Qdrant Vector Store \n",
    "def create_qdrant_store(chunks: List[Document], batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Create Qdrant Vector Store with embeddings using batching\n",
    "    Qdrant runs locally (no Docker needed!)\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of document chunks to embed\n",
    "        batch_size: Number of chunks to process at once (default: 100)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüîÑ Loading embedding model...\")\n",
    "    print(\"   (First time will download ~400MB model)\")\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Embedding model loaded: {EMBEDDING_MODEL}\")\n",
    "\n",
    "    import gc\n",
    "    gc.collect()  # Force garbage collection to close any lingering connections\n",
    "    \n",
    "    # Initialize Qdrant client\n",
    "    client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "    \n",
    "    # Delete existing collection safely\n",
    "    try:\n",
    "        client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "        print(f\"üóëÔ∏è  Deleted existing collection '{COLLECTION_NAME}'\")\n",
    "    except Exception:  # ‚úÖ Catch any exception (not just UnexpectedResponse)\n",
    "        pass  # Collection doesn't exist, that's fine\n",
    "    \n",
    "    print(f\"\\nüîÑ Creating Qdrant collection '{COLLECTION_NAME}'...\")\n",
    "    print(f\"   Processing {len(chunks)} chunks in batches of {batch_size}\\n\")\n",
    "    \n",
    "    # Initialize vector store with first batch\n",
    "    vector_store = None\n",
    "    batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(batches, desc=\"Creating embeddings\", unit=\"batch\")):\n",
    "        try:\n",
    "            if batch_idx == 0:\n",
    "                # First batch: create collection\n",
    "                vector_store = Qdrant.from_documents(\n",
    "                    documents=batch,\n",
    "                    embedding=embeddings,\n",
    "                    url=\"http://localhost:6333\",\n",
    "                    collection_name=COLLECTION_NAME,\n",
    "                    force_recreate=True\n",
    "                )\n",
    "            else:\n",
    "                # Subsequent batches: add to existing collection\n",
    "                vector_store.add_documents(batch)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error in batch {batch_idx + 1}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ Qdrant collection created successfully!\")\n",
    "    print(f\"   Location: {QDRANT_PATH}\")\n",
    "    print(f\"   Collection: {COLLECTION_NAME}\")\n",
    "    print(f\"   Total vectors: {len(chunks)}\")\n",
    "    print(f\"   Batches processed: {len(batches)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return vector_store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPACE-BIOLOGY-KNOWLEDGE-ENGINE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
